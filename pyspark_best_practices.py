# -*- coding: utf-8 -*-
"""PySpark Best Practices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FpeDhIrdbg3As__JfYjhl0LPU54vtl7k

# 🚀 Introduction to PySpark
PySpark is the Python API for Apache Spark — a distributed data processing engine ideal for large-scale data handling.

It's built for **parallel computation**, **cluster execution**, and **fault tolerance**, making it the go-to tool for **Big Data** workloads.

We'll now walk through real-life use cases, highlighting PySpark's syntax and advantages.
"""

# ✅ Install and Set Up PySpark (Only needed on Colab or local setup)
!pip install pyspark --quiet

# ✅ Import PySpark and Start a Session
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder \
    .appName("PySpark Essentials") \
    .getOrCreate()

import pyspark
print(pyspark.__version__)

"""🧱 DataFrame Creation"""

# ▶️ Create DataFrame from Dict
data = [{"col1": 1, "col2": "x"}, {"col1": 2, "col2": "y"}, {"col1": 3, "col2": "z"}]
df = spark.createDataFrame(data)
df.show()

"""Creating Some Synthetic Data"""

from sklearn.datasets import make_regression
import pandas as pd
# Create synthetic finance-related data
# Let's imagine features like 'stock_price_yesterday', 'interest_rate', 'inflation_rate',
# 'volume', 'market_sentiment', and a target 'stock_price_today'.
X, y = make_regression(n_samples=300, n_features=10, random_state=42, n_informative=8, noise=10)

# Convert to pandas DataFrame
column_names = [f'feature_{i+1}' for i in range(10)]
df_finance = pd.DataFrame(X, columns=column_names)
df_finance['target_stock_price'] = y

# Add some columns that might be more finance-specific (even if synthetic)
df_finance['stock_price_yesterday'] = df_finance['feature_1'] * 100 + 500 # Example calculation
df_finance['interest_rate'] = df_finance['feature_2'] * 0.1 + 2.0 # Example calculation
df_finance['inflation_rate'] = df_finance['feature_3'] * 0.05 + 1.0 # Example calculation

# Drop some of the original 'feature' columns to keep it around 10
df_finance = df_finance[['stock_price_yesterday', 'interest_rate', 'inflation_rate',
                         'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8',
                         'feature_9', 'target_stock_price']]

# Save to CSV
csv_file_path = 'finance_data.csv'
df_finance.to_csv(csv_file_path, index=False)
# Save to CSV
csv_file_path = 'finance_data.xlsx'
df_finance.to_excel(csv_file_path, index=False)
#Save to DB
import sqlite3
conn = sqlite3.connect('finance_data.db')
# Save the pandas DataFrame to a SQLite table
df_finance.to_sql('finance_data', conn, if_exists='replace', index=False)
conn.close()
print("DataFrame 'df_finance' successfully saved to 'finance_data.db' as table 'finance_data'.")

print(f"Synthetic finance data saved to {csv_file_path}")
print(df_finance.head())

"""📥 Reading Data"""

# 📁 Read CSV
df_csv = spark.read.csv("/content/finance_data.csv", header=True, inferSchema=True)
df_csv.show()

# 📁 Read Excel (via pandas workaround, PySpark doesn’t natively support Excel)
import pandas as pd
pdf = pd.read_excel("/content/finance_data.xlsx")
df_excel = spark.createDataFrame(pdf)
df_excel.show()

# 🛢️ Read from Database (via JDBC)
# jdbc_url = "jdbc:sqlite:example.db"
# df_db = spark.read.format("jdbc") \
#     .option("url", jdbc_url) \
#     .option("dbtable", "users") \
#     .load()
# df_db.show()

"""💾 Writing Data"""

# 💾 Write to CSV
df_csv_output_path = "df_csv_output"
df_csv.write.csv(df_csv_output_path, header=True, mode="overwrite")

# Read the saved CSV file back into a new DataFrame
df_csv_read = spark.read.csv(df_csv_output_path, header=True, inferSchema=True)

# Show the content of the read DataFrame
df_csv_read.show()

# 💾 Write to Parquet (Preferred for performance)
df_excel_csv_output_path = "df_excel_output"
df_excel.write.csv(df_excel_csv_output_path, header=True, mode="overwrite")

# Read the saved CSV file back into a new DataFrame
df_excel_read = spark.read.csv(df_excel_csv_output_path, header=True, inferSchema=True)

# Show the content of the read DataFrame
df_excel_read.show()

# # 💾 Write to Database
# df.write.format("jdbc") \
#     .option("url", jdbc_url) \
#     .option("dbtable", "new_table") \
#     .mode("overwrite") \
#     .save()

"""🔍 Inspect DataFrame"""

df_csv.show(5)  # View first 5 rows

df_csv.tail(5)  # View last 5 rows (pandas-like workaround needed)

df_csv.printSchema()

df_csv.describe().show()

df_csv.columns

df_csv.count()

"""🧹 Handling Nulls"""

df_csv.select([count(when(col(c).isNull(), c)).alias(c) for c in df_csv.columns]).show()

df_csv.dropna().show()

df_csv.fillna(0).show()

"""🧠 Selecting and Filtering"""

df.select("col1").show()

df.select(["col1", "col2"]).show()

df.filter(df.col1 > 1).show()

df.filter(df.col2.contains("y")).show()

df.limit(2).show()

"""🧼 Data Cleaning / Transformations"""

# Convert String to Date
df_dates = df.withColumn("date", to_date(lit("2024-06-01")))

df_dates.show()

# Cast data types
df.withColumn("col1_int", col("col1").cast("int")).show()

# String ops
df.withColumn("lower_col2", lower(col("col2"))).show()

df.withColumn("trimmed_col2", trim(col("col2"))).show()

df.withColumn("replaced_col2", regexp_replace("col2", "x", "xx")).show()

"""🆎 Rename & New Columns"""

# Rename columns
df_renamed = df.withColumnRenamed("col1", "new_col1")

df_renamed.show()

# Create new column
df.withColumn("col1_double", col("col1") * 2).show()

# Apply UDF
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

@udf(IntegerType())
def add_ten(x):
    return x + 10

df.withColumn("plus_10", add_ten("col1")).show()

"""🧮 GroupBy and Aggregation"""

df_group = spark.createDataFrame(
    [("a", 10), ("a", 20), ("b", 30), ("b", 40)],
    ["col", "value"]
)
df_group.groupBy("col").agg(mean("value").alias("mean_val")).show()

"""🔗 Joins and Unions"""

# Inner Join
df1 = spark.createDataFrame([(1, "x")], ["id", "val"])
df2 = spark.createDataFrame([(1, 100)], ["id", "score"])
df1.join(df2, on="id", how="inner").show()

# Concatenate
df1.union(df1).show()

"""📊 Pivot and Unpivot"""

# Pivot
df_pivot = spark.createDataFrame([
    ("NY", "HR", 1000),
    ("NY", "Tech", 1500),
    ("LA", "HR", 1100),
    ("LA", "Tech", 1600)
], ["city", "department", "salary"])

df_pivot.groupBy("city").pivot("department").agg(avg("salary")).show()

# Unpivot (Melt)
from pyspark.sql.functions import expr

df_unpivot = df_pivot.select("city", "department", "salary")
df_unpivot.selectExpr("city", "stack(2, 'HR', salary, 'Tech', salary) as (dept, val)").show()

"""⏱️ Datetime and Time Series"""

# Date extraction
df_dates.withColumn("year", year("date")).show()

# Window functions (rolling-like behavior)
from pyspark.sql.window import Window

w = Window.orderBy("date").rowsBetween(-2, 0)
df_time = df_dates.withColumn("rolling_avg", avg("col1").over(w))
df_time.show()

"""🔢 Frequency and Count"""

df_group.groupBy("col").count().show()

df_group.select("col").groupBy("col").count().show()

"""# 🧠 PySpark Exclusive Superpowers
- ⚙️ Distributed Computation across clusters (via SparkContext)
- 🔄 Lazy evaluation with DAG optimizations
- 🚀 Catalyst Optimizer for query optimization
- 🔥 Tungsten engine for physical execution
- ☁️ Native support for S3, Hive, HDFS, Delta Lake
- 🧩 Integration with MLlib, GraphX, Structured Streaming
- 🗃️ Columnar file formats (Parquet/ORC) optimized

💡 Use PySpark when:
- Handling 1M+ rows or GBs/TBs of data
- Need to scale computations across machines
- Doing data engineering or batch ETL pipelines

⚙️ 1. Distributed Computation via SparkContext

✅ Real-world use case: Use this to parallelize large data transformations (e.g., log processing, preprocessing in pipelines).
"""

# 💡 SparkContext gives access to the cluster and parallel computation features
sc = spark.sparkContext

# 🔁 Parallelize a list and perform a distributed map
rdd = sc.parallelize([1, 2, 3, 4, 5])
squared = rdd.map(lambda x: x ** 2).collect()
print("Squared values using distributed RDD:", squared)

"""🔄 2. Lazy Evaluation & DAG Optimization

⚠️ PySpark doesn't execute anything until an action like .show(), .collect(), or .write() is called. This helps Spark optimize the entire execution pipeline.
"""

# 💤 All transformations are lazy and build a Directed Acyclic Graph (DAG)
df_lazy = df.filter(col("col1") > 1).select("col2")

# 🔍 This triggers the execution
df_lazy.show()

"""🚀 3. Catalyst Optimizer for Query Optimization

🔥 Catalyst Optimizer rewrites your SQL/DataFrame queries internally for best performance (predicate pushdown, constant folding, etc.)
"""

# 👀 Check the optimized query plan
df_group.groupBy("col").agg(avg("value")).explain()

"""🔥 4. Tungsten Engine for Physical Execution

⚙️ Observe WholeStageCodegen, ColumnarToRow, and other Tungsten execution optimizations in the plan.
"""

# Tungsten is Spark's memory and execution engine
# It handles bytecode generation, cache-aware computation, and optimized memory usage.

# While not directly visible via code, it’s activated behind-the-scenes whenever transformations/actions run.

# For proof:
df_group.groupBy("col").agg(avg("value")).explain("formatted")

"""☁️ 5. Native Support for S3, HDFS, Hive, Delta Lake

⚡ Enables cloud-native data engineering, ETL pipelines, and massive-scale reads/writes.
"""

# 🔄 Reading from S3 (if properly configured with AWS credentials)
# df_s3 = spark.read.csv("s3a://my-bucket/path/data.csv")

# 🔄 Hive Table (only works if Hive metastore is connected)
# spark.sql("SELECT * FROM hive_db.my_table").show()

# 🔁 Delta Lake
# df_delta = spark.read.format("delta").load("s3a://my-bucket/delta-table")

# ✅ Universal support for Big Data filesystems: s3a://, hdfs://, abfs://, gs://

"""🧩 6. Integration with MLlib - Machine Learning

📈 Run distributed machine learning pipelines for huge datasets using pyspark.ml.
"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# Sample DataFrame for regression
df_ml = spark.createDataFrame([
    (1, 2.0), (2, 3.5), (3, 5.0), (4, 7.5), (5, 9.0)
], ["feature", "label"])

assembler = VectorAssembler(inputCols=["feature"], outputCol="features")
df_features = assembler.transform(df_ml)

# Fit Linear Regression
lr = LinearRegression(featuresCol="features", labelCol="label")
model = lr.fit(df_features)
model.summary.r2, model.coefficients

"""🌐 7. Structured Streaming - Real-time Data Processing

🧠 PySpark supports event-time processing, windowed aggregations, exactly-once semantics using readStream / writeStream.
"""

# Simulate streaming by reading a directory continuously
# You can run this in a real Spark cluster with a streaming source

# streaming_df = spark.readStream \
#     .schema(df.schema) \
#     .option("maxFilesPerTrigger", 1) \
#     .csv("streaming_data/")

# query = streaming_df.writeStream \
#     .outputMode("append") \
#     .format("console") \
#     .start()

# query.awaitTermination()

"""🗃️ 8. Columnar File Formats - Parquet & ORC

🧩 These formats are splittable, compressed, and fast for column-wise operations, ideal for analytics & queries.
"""

# 💾 Write and read Parquet (default for performance)
df.write.parquet("data.parquet", mode="overwrite")
df_parquet = spark.read.parquet("data.parquet")
df_parquet.show()

# 💾 ORC (optimized row columnar)
df.write.orc("data.orc", mode="overwrite")
df_orc = spark.read.orc("data.orc")
df_orc.show()

